\documentclass{article}
\usepackage{fullpage}

%load needed packages
\usepackage{graphicx}
\usepackage{array}
\usepackage{booktabs}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{hyperref}


\usepackage{float}  % Necesario para [H]
\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{HTML}{5AB2FF}
\definecolor{morado}{HTML}{AD88C6}
\definecolor{BG}{HTML}{EEEEEE}
\definecolor{azul}{HTML}{4D869C}
\definecolor{sqlblue}{HTML}{FF8C00} % Color para las palabras clave SQL
\usepackage{listings}
\usepackage{xcolor}


%estilo python
\usepackage{xcolor}

% Define the colors for the style
\definecolor{BG}{rgb}{0.95,0.95,0.95}  % Background color
\definecolor{keywordcolor}{rgb}{0.0,0.0,1.0} % Blue for keywords
\definecolor{commentcolor}{rgb}{0.0,0.5,0.0} % Green for comments
\definecolor{stringcolor}{rgb}{1.0,0.0,0.0}  % Red for strings
\definecolor{attributecolor}{rgb}{0.8,0.3,0.8} % Purple for attributes
\definecolor{importcolor}{rgb}{0.0,0.6,0.6} % Teal for import statements

% Define the style for Python code
\lstdefinestyle{mypython}{
	backgroundcolor=\color{BG},   % Background color
	basicstyle=\footnotesize\ttfamily,  
	breaklines=true,                  
	language=Python,                  
	keywordstyle=\color{keywordcolor},    
	commentstyle=\color{commentcolor}, 
	stringstyle=\color{stringcolor},
	frame=shadowbox, 
	morekeywords={model},  % Add 'model' to keywords
	keywordstyle=[2]\color{importcolor}, % Color for import statements
	sensitive=true,       % Case sensitive
	morecomment=[s]{"""}{"} % Allows for multi-line strings
}



\lstset{style=mypython}
% Estilo para DDL
\lstdefinestyle{ddlstyle}{
	language=SQL,
	backgroundcolor=\color{BG},
	commentstyle=\color{codegreen},
	basicstyle=\ttfamily\small,
	keywordstyle=\color{azul},
	stringstyle=\color{morado},
	showstringspaces=false,
	breaklines=true,
	frame=shadowbox,
	numbers=left,
	numberstyle=\tiny\color{gray},
	captionpos=b,
}

% Estilo para SQL
\lstdefinestyle{sqlstyle}{
	language=SQL,
	backgroundcolor=\color{BG},
	commentstyle=\color{codegreen},
	basicstyle=\ttfamily\small,
	keywordstyle=\color{sqlblue}, % Color diferente para palabras clave SQL
	stringstyle=\color{morado},
	showstringspaces=false,
	breaklines=true,
	frame=shadowbox,
	numbers=left,
	numberstyle=\tiny\color{gray},
	captionpos=b,
}

\begin{document}



% Portada
\begin{titlepage}
	\centering
	\vspace*{3cm}
	
	% Título destacado
	{\Huge \textbf{Lab 2}\\[0.5cm]}

	{\Huge \textbf{Task B: Supervised Learning}\\[0.5cm]}
	
	% Espacio y logotipo (si lo tienes, por ejemplo el logo de tu universidad)
	\vspace{2cm}
	\includegraphics[width=0.3\textwidth]{images/uma_logo.jpg}\\[1cm]
	
	% Nombre del autor
	{\LARGE \textbf{Alejandro Silva Rodríguez}\\[0.5cm]}
	{\LARGE \textbf{Marta Cuevas Rodríguez}\\[0.5cm]}
	{\large \textit{Aprendizaje Computacional}\\
		Universidad de Málaga\\
		}
	
	\vfill
	
	% Fecha en la parte inferior de la página
	{\large October 2024}
\end{titlepage}

% indice
\tableofcontents

\newpage

\section{Introduction}

This project explores both fundamental and advanced concepts in supervised learning, placing a particular emphasis on the significant potential of Deep Learning in biomedical fields. Supervised learning, a technique where models learn to classify or predict outcomes based on labeled data, is foundational in machine learning, particularly for applications involving complex classification tasks. However, with the advent of Deep Learning, models now have the capacity to process and analyze high-dimensional data with enhanced accuracy and flexibility, making them highly effective in medical and health-related applications.

In this project, we begin by examining essential concepts, such as partitioning datasets into training and testing sets to evaluate model generalizability, and the use of cross-validation to optimize model performance. We then delve into the structure of neural networks, differentiating traditional feed-forward networks from more advanced architectures used in Deep Learning. These modern architectures—characterized by multiple hidden layers, complex connections, and sophisticated activation functions—enable models to learn hierarchical representations of data, capturing intricate patterns and dependencies.

Moreover, this project addresses critical challenges that arise when applying Deep Learning to biomedical contexts. These include the risk of overfitting, where models may perform well on training data but fail to generalize to new data, and the common issue of data scarcity, given the limited availability of labeled biomedical datasets. We will explore techniques to alleviate these challenges, such as regularization, data augmentation, and synthetic data generation, which expand the training dataset and improve model robustness.

Finally, we will review recent advancements in Deep Learning applied to healthcare, showcasing examples where such models have significantly improved diagnostics, disease prediction, and patient outcomes. This project aims to build a comprehensive understanding of how supervised learning, and Deep Learning in particular, can drive impactful changes in the healthcare sector.

\subsection{Objectives}

\begin{itemize}
	\item \textbf{Develop a foundational understanding of supervised learning techniques}: Explore how supervised learning methods work, emphasizing their role in solving classification tasks in healthcare.
	
	\item \textbf{Examine Deep Learning concepts and architectures}: Differentiate traditional feed-forward networks from advanced Deep Learning architectures, analyzing the benefits of additional hidden layers, complex connections, and hierarchical data representation.
	
	\item \textbf{Understand data partitioning and evaluation}: Learn the importance of splitting datasets into training and test sets to ensure model performance generalizes to new data, and explore cross-validation as a technique to further enhance reliability and robustness.
	
	\item \textbf{Implement overfitting prevention techniques}: Identify strategies to reduce overfitting, such as regularization, dropout, and early stopping, ensuring that models perform well not only on training data but also on new, unseen data.
	
	\item \textbf{Address data scarcity through data augmentation and synthetic data generation}: Explore how data scarcity impacts biomedical applications and implement data augmentation and synthetic data generation techniques to expand limited datasets and enhance model accuracy.
	
	\item \textbf{Analyze real-world applications of Deep Learning in healthcare}: Review case studies where Deep Learning has been successfully applied in the medical field, such as diagnostics and predictive models for disease progression, highlighting its potential to transform patient care.
\end{itemize}



\section{Supervised Learning}

Supervised learning is a foundational approach within machine learning, widely researched and applied across various fields, including multimedia content processing. The core feature that distinguishes supervised learning is its use of annotated training data, where each example is paired with a known label, often representing a class in classification tasks. This setup is akin to having a “supervisor” guiding the learning system by providing correct associations for each training sample \cite{Cunningham2008}. 

Through supervised learning algorithms, models are derived from the training data to enable the classification of new, unlabeled examples. These models are trained with the aim of minimizing prediction errors—a concept underpinned by the theory of risk minimization. Techniques such as support vector machines and nearest neighbor classifiers, two highly popular algorithms in multimedia and other domains, exemplify the methods utilized to implement supervised learning, as they effectively leverage labeled data to create accurate and adaptable models \cite{Cunningham2008}.\\

\subsection{Importance of Dataset Splitting in Supervised Learning}

Dividing a dataset into training and test sets is a critical step in developing supervised learning models, especially in fields such as biomedical classification. This separation ensures that the model's performance is evaluated on data it has not encountered before, providing an honest assessment of its generalization ability \cite{https://doi.org/10.1155/2020/2836236}. Careful consideration of sampling methods during the training and testing stages directly impacts the system's stability and performance, as inappropriate sample selection can lead to misleading evaluations and potential overfitting.


\subsection{Cross-Validation in Model Evaluation and Selection}

Cross-validation is a widely-used data resampling technique in model evaluation and selection, serving multiple purposes, including hyperparameter tuning, preventing overfitting, and estimating the generalization error of predictive models. By partitioning the dataset into multiple subsets or "folds," cross-validation allows a model to train on various splits and validate on the remaining portions, leading to a more reliable performance estimate \cite{berrar2019cross}.
\\

Among the most common cross-validation methods are k-fold cross-validation, leave-one-out cross-validation (LOOCV), and nested cross-validation. In particular, nested cross-validation is valuable for tuning hyperparameters, as it involves two layers of cross-validation: the outer loop for testing and the inner loop for training and validation. This approach is especially beneficial in complex models where a single validation loop might underestimate the generalization error.
\\

Cross-validation is crucial for enhancing models, particularly by mitigating overfitting and enabling robust comparisons among learning algorithms. In deep learning, however, standard cross-validation is less common due to the high computational cost. Instead, methods such as single hold-out validation or using a dedicated validation set within an iterative training process are more frequently used, allowing large models to be evaluated without prohibitive resource requirements \cite{berrar2019cross}.Cross-validation is less commonly used in deep learning due to the high computational costs, especially with large datasets and deep networks. Instead, deep learning models often use a fixed validation set, allowing for efficient hyperparameter tuning and performance evaluation without the resource demands of full cross-validation.


\subsection{Artificial Neural Networks}

Artificial Neural Networks (ANNs) are computational models inspired by the complex structure and functioning of the human brain, where billions of interconnected neurons process information simultaneously. Just as the brain can recognize patterns, make decisions, and process language, ANNs have been developed to perform tasks like language translation, image recognition, and time series forecasting. ANNs operate as universal function approximators, meaning they can map input data to outputs in a highly flexible manner, making them suitable for a wide variety of applications \cite{wang2003artificial}.
\\

An ANN consists of multiple layers of artificial neurons or nodes, which are organized in an architecture of interconnected layers. These layers typically include an input layer, one or more hidden layers, and an output layer. Each neuron in a given layer processes the input it receives, applies a weighted sum and an activation function, and passes the output to the next layer. Through training, these weights are adjusted to minimize the difference between the network’s prediction and the true output, allowing the ANN to "learn" patterns in data.





\section{Deep learning}
Deep Learning (DL) is a subset of machine learning that focuses on the use of neural networks with many layers to model complex patterns in large datasets. Unlike traditional machine learning methods, which often require feature engineering, DL can automatically extract hierarchical representations from raw data. This capability is especially beneficial when dealing with high-dimensional data, such as images and sequences. 
\\

One of the most significant advancements in DL is the introduction of models like Transformers. Transformers, introduced by Vaswani et al. in 2017, leverage an attention mechanism that allows them to weigh the importance of different parts of the input data dynamically. This architecture has led to remarkable performance improvements in various tasks, including natural language processing and image recognition, by efficiently capturing long-range dependencies in data without the limitations of recurrent architectures \cite{vaswani2017attention}.

\subsection{DL Models VS Traditional Feedforward Neural Networks}
Deep Learning models represent a leap forward from traditional feedforward neural networks. While feedforward networks consist of simple, layered architectures that process data in a linear manner, DL models, particularly convolutional neural networks (CNNs) and recurrent neural networks (RNNs), incorporate multiple layers and complex structures that enable them to learn more intricate patterns. 
\\

CNNs excel in processing grid-like data, such as images, by using convolutional layers to automatically detect features like edges and textures. On the other hand, RNNs are designed for sequential data, allowing them to maintain a memory of previous inputs, which is crucial for tasks like time series analysis or language modeling. The introduction of Transformers further enhances these capabilities, allowing for parallel processing of data, which significantly speeds up training and improves performance on a range of tasks, from translation to image classification.

\subsection{Remarkable Applications of DL Models by Google}
Google has been at the forefront of applying DL models across various domains, achieving groundbreaking results. Two notable examples include:

\begin{itemize}
	\item \textbf{AlphaFold:} Developed by DeepMind, AlphaFold addresses the complex problem of protein folding. By predicting the 3D structures of proteins from their amino acid sequences, AlphaFold has revolutionized our understanding of biology, paving the way for advancements in drug discovery and disease research \cite{jumper2021highly}.
	
	\item \textbf{Gemini Project:} Gemini is an advanced multimodal language model developed by Google, similar to ChatGPT and LLaMA. As a multimodal model, Gemini can process and generate various types of information, including text, images, and code. This capability allows for a deeper understanding and more creative applications. For example, Gemini can describe images with text, generate images based on text descriptions, and even translate between different languages while considering the visual context. \cite{buscemi2024chatgpt}.
\end{itemize}

\subsection{Overfitting in DL Models}
Overfitting occurs when a model learns the training data too well, including its noise and outliers, resulting in poor performance on unseen data. This situation arises when a model becomes too complex relative to the amount of training data available. 
\\

To combat overfitting, deep learning models employ several strategies. Techniques such as dropout, which randomly disables a subset of neurons during training, and regularization methods, which penalize overly complex models, are commonly used. Moreover, data augmentation—where existing training data is transformed through techniques like rotation, flipping, or adding noise—helps create a more robust model by exposing it to a wider variety of inputs during training.

\subsection{Addressing Data Scarcity in Biomedical Applications}
The scarcity of labeled data poses a significant challenge in applying deep learning to biomedical problems. To mitigate this issue, several techniques can be employed:

\begin{itemize}
	\item \textbf{Data Augmentation:} By applying statistical transformations to existing data, such as flipping or rotating images, we can artificially expand the training dataset, improving the model's ability to generalize.
	
	\item \textbf{Generative Adversarial Networks (GANs):} GANs consist of two competing neural networks that generate synthetic data resembling the training data. This approach has been successful in creating realistic biomedical images, thus addressing the issue of data scarcity.
	
	\item \textbf{Transfer Learning:} By leveraging pre-trained models on large datasets, we can fine-tune these models on smaller, domain-specific datasets. This method helps overcome data limitations and accelerates model training, providing significant benefits in fields where data collection is challenging.
\end{itemize}


\section{Repository Access}

All additional information, including the source code and full documentation of this project, is available in the GitHub repository \cite{cuevas2024github}.


% Incluir la bibliografía
\bibliographystyle{plain}  % Estilo de la bibliografía (por ejemplo, plain, alpha, ieee, etc.)
\bibliography{bibli}  % Nombre del archivo .bib sin la extensión

\end{document}
